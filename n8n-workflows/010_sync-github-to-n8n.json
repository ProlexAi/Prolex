{
  "name": "GitHub ‚Üí n8n Sync v2026 (Ultra-Robust)",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "github-sync-v2",
        "responseMode": "onReceived",
        "options": {
          "rawBody": true
        }
      },
      "id": "webhook-github-push",
      "name": "üì• GitHub Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [240, 400],
      "webhookId": "github-sync-v2",
      "notes": "Endpoint: POST /webhook/github-sync-v2\nRe√ßoit les push events GitHub avec validation HMAC"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * VALIDATION & PARSING DU PAYLOAD GITHUB\n * - V√©rifie la signature HMAC\n * - Valide la structure du payload\n * - Extrait les m√©tadonn√©es essentielles\n */\n\nconst crypto = require('crypto');\n\nconst executionId = $execution.id;\nconst timestamp = new Date().toISOString();\nconst payload = $input.item.json.body || $input.item.json;\nconst headers = $input.item.json.headers || {};\n\n// Configuration\nconst GITHUB_WEBHOOK_SECRET = $env.GITHUB_WEBHOOK_SECRET || '';\nconst ENABLE_SIGNATURE_VALIDATION = ($env.ENABLE_SIGNATURE_VALIDATION || 'true') === 'true';\n\n// Fonction de logging structur√©\nfunction log(level, message, data = {}) {\n  return {\n    timestamp,\n    execution_id: executionId,\n    level,\n    message,\n    ...data\n  };\n}\n\n// 1. Validation de la signature HMAC (s√©curit√©)\nif (ENABLE_SIGNATURE_VALIDATION && GITHUB_WEBHOOK_SECRET) {\n  const signature = headers['x-hub-signature-256'];\n  if (!signature) {\n    return {\n      json: {\n        ...log('ERROR', 'Missing GitHub signature', {\n          error_type: 'security_validation_failed',\n          error_message: 'No x-hub-signature-256 header found',\n          status: 'rejected'\n        })\n      }\n    };\n  }\n  \n  const rawBody = $input.item.json.rawBody || JSON.stringify(payload);\n  const hmac = crypto.createHmac('sha256', GITHUB_WEBHOOK_SECRET);\n  const digest = 'sha256=' + hmac.update(rawBody).digest('hex');\n  \n  if (signature !== digest) {\n    return {\n      json: {\n        ...log('ERROR', 'Invalid GitHub signature', {\n          error_type: 'security_validation_failed',\n          error_message: 'HMAC signature mismatch',\n          status: 'rejected'\n        })\n      }\n    };\n  }\n}\n\n// 2. Validation de la structure du payload\nif (!payload.repository || !payload.ref) {\n  return {\n    json: {\n      ...log('ERROR', 'Invalid payload structure', {\n        error_type: 'payload_validation_failed',\n        error_message: 'Missing required fields: repository or ref',\n        status: 'rejected'\n      })\n    }\n  };\n}\n\n// 3. Extraction des m√©tadonn√©es\nconst repo = payload.repository.full_name || 'unknown';\nconst ref = payload.ref || '';\nconst branch = ref.replace('refs/heads/', '');\nconst pusher = payload.pusher?.name || payload.sender?.login || 'unknown';\nconst commits = payload.commits || [];\nconst eventId = headers['x-github-delivery'] || executionId;\n\n// 4. D√©terminer l'environnement cible selon la branche\nlet targetEnv = 'dev';\nlet n8nBaseUrl = $env.N8N_DEV_URL || 'http://localhost:5678';\nlet n8nApiKey = $env.N8N_DEV_API_KEY || $env.N8N_API_KEY;\n\nif (branch === 'main' || branch === 'master') {\n  targetEnv = 'prod';\n  n8nBaseUrl = $env.N8N_PROD_URL || n8nBaseUrl;\n  n8nApiKey = $env.N8N_PROD_API_KEY || n8nApiKey;\n} else if (branch === 'staging') {\n  targetEnv = 'staging';\n  n8nBaseUrl = $env.N8N_STAGING_URL || n8nBaseUrl;\n  n8nApiKey = $env.N8N_STAGING_API_KEY || n8nApiKey;\n}\n\n// 5. Stocker le contexte pour les nodes suivants\nreturn {\n  json: {\n    ...log('INFO', 'Webhook validated successfully', {\n      event_id: eventId,\n      repo,\n      branch,\n      target_env: targetEnv,\n      pusher,\n      commit_count: commits.length,\n      n8n_base_url: n8nBaseUrl,\n      n8n_api_key_configured: !!n8nApiKey,\n      status: 'validated'\n    }),\n    _context: {\n      payload,\n      n8nBaseUrl,\n      n8nApiKey,\n      commits\n    }\n  }\n};"
      },
      "id": "validate-and-parse",
      "name": "üîê Validate & Parse",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [460, 400],
      "notes": "Validation HMAC + extraction m√©tadonn√©es + routing multi-env"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "leftValue": "={{ $json.status }}",
              "rightValue": "validated",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "filter-valid-only",
      "name": "‚úÖ Filter Valid Only",
      "type": "n8n-nodes-base.filter",
      "typeVersion": 2,
      "position": [680, 400]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * EXTRACTION DES FICHIERS MODIFI√âS\n * - Parse tous les commits du push\n * - Filtre uniquement n8n-workflows/*.json\n * - D√©duplique les fichiers\n * - Ajoute m√©tadonn√©es de changement\n */\n\nconst context = $input.item.json._context;\nconst metadata = $input.item.json;\nconst commits = context.commits || [];\n\nconst filesChanged = [];\nconst seenFiles = new Set();\n\nfor (const commit of commits) {\n  const commitSha = commit.id;\n  const commitMessage = commit.message || '';\n  const commitAuthor = commit.author?.username || commit.author?.name || metadata.pusher;\n  const commitTimestamp = commit.timestamp || new Date().toISOString();\n  \n  // Helper pour ajouter un fichier\n  const addFile = (filepath, changeType) => {\n    if (!filepath.startsWith('n8n-workflows/')) return;\n    if (!filepath.endsWith('.json')) return;\n    if (seenFiles.has(filepath)) return;\n    \n    filesChanged.push({\n      file_path: filepath,\n      change_type: changeType,\n      repo: metadata.repo,\n      branch: metadata.branch,\n      target_env: metadata.target_env,\n      commit_sha: commitSha,\n      commit_message: commitMessage,\n      commit_timestamp: commitTimestamp,\n      actor: commitAuthor,\n      event_id: metadata.event_id\n    });\n    seenFiles.add(filepath);\n  };\n  \n  // Fichiers ajout√©s\n  (commit.added || []).forEach(f => addFile(f, 'added'));\n  \n  // Fichiers modifi√©s\n  (commit.modified || []).forEach(f => addFile(f, 'modified'));\n  \n  // Fichiers supprim√©s\n  (commit.removed || []).forEach(f => addFile(f, 'removed'));\n}\n\n// Si aucun fichier workflow n'a chang√©, retourner un item sp√©cial\nif (filesChanged.length === 0) {\n  return [{\n    json: {\n      file_path: 'none',\n      change_type: 'none',\n      skip: true,\n      repo: metadata.repo,\n      branch: metadata.branch,\n      target_env: metadata.target_env,\n      event_id: metadata.event_id,\n      message: 'No n8n workflow files changed in this push',\n      _context: context\n    }\n  }];\n}\n\n// Retourner un item par fichier\nreturn filesChanged.map(file => ({\n  json: {\n    ...file,\n    skip: false,\n    _context: context\n  }\n}));"
      },
      "id": "extract-files",
      "name": "üìÑ Extract Changed Files",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [900, 400]
    },
    {
      "parameters": {
        "batchSize": 1,
        "options": {}
      },
      "id": "split-batches",
      "name": "üîÑ Process One by One",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [1120, 400],
      "notes": "Traite chaque fichier individuellement pour √©viter les timeouts"
    },
    {
      "parameters": {
        "mode": "rules",
        "rules": {
          "rules": [
            {
              "conditions": {
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict"
                },
                "conditions": [
                  {
                    "leftValue": "={{ $json.skip }}",
                    "rightValue": "true",
                    "operator": {
                      "type": "boolean",
                      "operation": "true"
                    }
                  }
                ],
                "combinator": "and"
              },
              "renameOutput": true,
              "outputKey": "skip"
            },
            {
              "conditions": {
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict"
                },
                "conditions": [
                  {
                    "leftValue": "={{ $json.change_type }}",
                    "rightValue": "removed",
                    "operator": {
                      "type": "string",
                      "operation": "equals"
                    }
                  }
                ],
                "combinator": "and"
              },
              "renameOutput": true,
              "outputKey": "removed"
            }
          ]
        },
        "options": {}
      },
      "id": "route-by-change-type",
      "name": "üîÄ Route by Type",
      "type": "n8n-nodes-base.switch",
      "typeVersion": 3,
      "position": [1340, 400]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * T√âL√âCHARGEMENT FICHIER DEPUIS GITHUB (AVEC RETRY)\n * - Retry automatique 3x avec exponential backoff\n * - Timeout configurable\n * - Validation du contenu\n */\n\nconst item = $input.item.json;\nconst context = item._context;\n\nconst repo = item.repo;\nconst branch = item.branch;\nconst filepath = item.file_path;\n\nconst GITHUB_API_BASE = 'https://api.github.com';\nconst GITHUB_TOKEN = $env.GITHUB_TOKEN || $env.GITHUB_API_TOKEN;\nconst MAX_RETRIES = parseInt($env.GITHUB_MAX_RETRIES || '3');\nconst TIMEOUT_MS = parseInt($env.GITHUB_TIMEOUT_MS || '10000');\n\nif (!GITHUB_TOKEN) {\n  return {\n    json: {\n      ...item,\n      error: true,\n      error_type: 'config_missing',\n      error_message: 'GITHUB_TOKEN not configured',\n      retry_count: 0\n    }\n  };\n}\n\n// Fonction retry avec exponential backoff\nasync function fetchWithRetry(url, options, maxRetries) {\n  let lastError;\n  \n  for (let attempt = 0; attempt <= maxRetries; attempt++) {\n    try {\n      const controller = new AbortController();\n      const timeoutId = setTimeout(() => controller.abort(), TIMEOUT_MS);\n      \n      const response = await fetch(url, {\n        ...options,\n        signal: controller.signal\n      });\n      \n      clearTimeout(timeoutId);\n      \n      if (!response.ok) {\n        throw new Error(`HTTP ${response.status}: ${response.statusText}`);\n      }\n      \n      return await response.json();\n      \n    } catch (error) {\n      lastError = error;\n      \n      if (attempt < maxRetries) {\n        // Exponential backoff: 2^attempt * 1000ms\n        const delayMs = Math.pow(2, attempt) * 1000;\n        await new Promise(resolve => setTimeout(resolve, delayMs));\n      }\n    }\n  }\n  \n  throw lastError;\n}\n\n// T√©l√©charger le fichier\ntry {\n  const url = `${GITHUB_API_BASE}/repos/${repo}/contents/${filepath}?ref=${branch}`;\n  \n  const fileData = await fetchWithRetry(url, {\n    headers: {\n      'Authorization': `token ${GITHUB_TOKEN}`,\n      'Accept': 'application/vnd.github.v3+json',\n      'User-Agent': 'n8n-prolex-sync'\n    }\n  }, MAX_RETRIES);\n  \n  // Valider la r√©ponse\n  if (!fileData.content || fileData.encoding !== 'base64') {\n    return {\n      json: {\n        ...item,\n        error: true,\n        error_type: 'invalid_response',\n        error_message: 'GitHub returned invalid file data (missing content or not base64)',\n        retry_count: 0\n      }\n    };\n  }\n  \n  return {\n    json: {\n      ...item,\n      github_file_data: fileData,\n      github_sha: fileData.sha,\n      github_size: fileData.size,\n      error: false,\n      retry_count: 0\n    }\n  };\n  \n} catch (error) {\n  return {\n    json: {\n      ...item,\n      error: true,\n      error_type: 'github_fetch_failed',\n      error_message: error.message,\n      retry_count: MAX_RETRIES\n    }\n  };\n}"
      },
      "id": "fetch-from-github",
      "name": "üì• Fetch from GitHub (Retry)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1560, 240]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * PARSE ET VALIDATION DU WORKFLOW JSON\n * - D√©code base64\n * - Parse JSON\n * - Valide structure n8n\n * - V√©rifie compatibilit√© version\n */\n\nconst item = $input.item.json;\n\n// Si erreur pr√©c√©dente, passer\nif (item.error) {\n  return { json: item };\n}\n\nconst fileData = item.github_file_data;\nconst encoding = fileData.encoding;\n\nif (encoding !== 'base64') {\n  return {\n    json: {\n      ...item,\n      error: true,\n      error_type: 'encoding_invalid',\n      error_message: `Expected base64 encoding, got: ${encoding}`\n    }\n  };\n}\n\ntry {\n  // D√©coder base64\n  const decoded = Buffer.from(fileData.content, 'base64').toString('utf-8');\n  \n  // Parser JSON\n  const workflowJson = JSON.parse(decoded);\n  \n  // Validation structure minimale\n  const requiredFields = ['name', 'nodes'];\n  const missingFields = requiredFields.filter(f => !workflowJson[f]);\n  \n  if (missingFields.length > 0) {\n    return {\n      json: {\n        ...item,\n        error: true,\n        error_type: 'workflow_validation_failed',\n        error_message: `Missing required fields: ${missingFields.join(', ')}`\n      }\n    };\n  }\n  \n  // Validation nodes\n  if (!Array.isArray(workflowJson.nodes) || workflowJson.nodes.length === 0) {\n    return {\n      json: {\n        ...item,\n        error: true,\n        error_type: 'workflow_validation_failed',\n        error_message: 'Workflow must have at least one node'\n      }\n    };\n  }\n  \n  // Extraire m√©tadonn√©es du workflow\n  const workflowMeta = {\n    name: workflowJson.name,\n    node_count: workflowJson.nodes.length,\n    has_webhook: workflowJson.nodes.some(n => n.type?.includes('webhook')),\n    has_schedule: workflowJson.nodes.some(n => n.type?.includes('schedule')),\n    tags: workflowJson.tags || []\n  };\n  \n  return {\n    json: {\n      ...item,\n      workflow_json: workflowJson,\n      workflow_meta: workflowMeta,\n      error: false\n    }\n  };\n  \n} catch (error) {\n  return {\n    json: {\n      ...item,\n      error: true,\n      error_type: 'json_parse_failed',\n      error_message: error.message\n    }\n  };\n}"
      },
      "id": "parse-workflow-json",
      "name": "üîç Parse & Validate JSON",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1780, 240]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * UPSERT WORKFLOW DANS N8N (AVEC RETRY)\n * - Cherche workflow existant par nom\n * - Create si nouveau, Update si existant\n * - Retry automatique sur erreurs r√©seau\n * - Validation post-upsert\n */\n\nconst item = $input.item.json;\n\n// Si erreur pr√©c√©dente, passer\nif (item.error) {\n  return {\n    json: {\n      ...item,\n      action_taken: 'skip_error',\n      workflow_id: null\n    }\n  };\n}\n\nconst context = item._context;\nconst workflowJson = item.workflow_json;\nconst workflowName = workflowJson.name;\nconst n8nBaseUrl = context.n8nBaseUrl;\nconst n8nApiKey = context.n8nApiKey;\n\nconst MAX_RETRIES = parseInt($env.N8N_MAX_RETRIES || '3');\nconst TIMEOUT_MS = parseInt($env.N8N_TIMEOUT_MS || '15000');\n\nif (!n8nApiKey) {\n  return {\n    json: {\n      ...item,\n      error: true,\n      error_type: 'config_missing',\n      error_message: 'N8N_API_KEY not configured',\n      action_taken: 'skip_config'\n    }\n  };\n}\n\n// Fonction retry\nasync function fetchWithRetry(url, options, maxRetries) {\n  let lastError;\n  \n  for (let attempt = 0; attempt <= maxRetries; attempt++) {\n    try {\n      const controller = new AbortController();\n      const timeoutId = setTimeout(() => controller.abort(), TIMEOUT_MS);\n      \n      const response = await fetch(url, {\n        ...options,\n        signal: controller.signal\n      });\n      \n      clearTimeout(timeoutId);\n      \n      if (!response.ok) {\n        const errorText = await response.text();\n        throw new Error(`HTTP ${response.status}: ${errorText}`);\n      }\n      \n      return await response.json();\n      \n    } catch (error) {\n      lastError = error;\n      \n      if (attempt < maxRetries) {\n        const delayMs = Math.pow(2, attempt) * 1000;\n        await new Promise(resolve => setTimeout(resolve, delayMs));\n      }\n    }\n  }\n  \n  throw lastError;\n}\n\ntry {\n  // 1. Chercher workflows existants\n  const searchUrl = `${n8nBaseUrl}/api/v1/workflows`;\n  const allWorkflows = await fetchWithRetry(searchUrl, {\n    method: 'GET',\n    headers: {\n      'X-N8N-API-KEY': n8nApiKey,\n      'Content-Type': 'application/json'\n    }\n  }, MAX_RETRIES);\n  \n  const existingWorkflow = allWorkflows.data?.find(w => w.name === workflowName);\n  \n  let result;\n  let actionTaken;\n  \n  if (existingWorkflow) {\n    // 2a. UPDATE existing\n    const updateUrl = `${n8nBaseUrl}/api/v1/workflows/${existingWorkflow.id}`;\n    const updateBody = {\n      ...workflowJson,\n      id: existingWorkflow.id\n    };\n    \n    result = await fetchWithRetry(updateUrl, {\n      method: 'PUT',\n      headers: {\n        'X-N8N-API-KEY': n8nApiKey,\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify(updateBody)\n    }, MAX_RETRIES);\n    \n    actionTaken = 'updated';\n    \n  } else {\n    // 2b. CREATE new\n    const createUrl = `${n8nBaseUrl}/api/v1/workflows`;\n    const createBody = { ...workflowJson };\n    delete createBody.id;\n    \n    result = await fetchWithRetry(createUrl, {\n      method: 'POST',\n      headers: {\n        'X-N8N-API-KEY': n8nApiKey,\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify(createBody)\n    }, MAX_RETRIES);\n    \n    actionTaken = 'created';\n  }\n  \n  const workflowId = result.data?.id || result.id;\n  \n  return {\n    json: {\n      ...item,\n      action_taken: actionTaken,\n      workflow_id: workflowId,\n      workflow_url: `${n8nBaseUrl}/workflow/${workflowId}`,\n      error: false,\n      sync_status: 'success'\n    }\n  };\n  \n} catch (error) {\n  return {\n    json: {\n      ...item,\n      error: true,\n      error_type: 'n8n_upsert_failed',\n      error_message: error.message,\n      action_taken: 'failed',\n      sync_status: 'failed'\n    }\n  };\n}"
      },
      "id": "upsert-workflow-n8n",
      "name": "üöÄ Upsert to n8n (Retry)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [2000, 240]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * DISABLE WORKFLOW (FICHIER SUPPRIM√â)\n * - Recherche workflow par nom\n * - D√©sactive ou supprime selon config\n * - Retry automatique\n */\n\nconst item = $input.item.json;\nconst context = item._context;\n\nconst filepath = item.file_path;\nconst filename = filepath.split('/').pop().replace('.json', '');\nconst workflowNameGuess = filename.replace(/_/g, ' ').replace(/-/g, ' ');\n\nconst n8nBaseUrl = context.n8nBaseUrl;\nconst n8nApiKey = context.n8nApiKey;\nconst DELETE_ON_REMOVE = ($env.DELETE_WORKFLOWS_ON_REMOVE || 'false') === 'true';\nconst MAX_RETRIES = parseInt($env.N8N_MAX_RETRIES || '3');\nconst TIMEOUT_MS = parseInt($env.N8N_TIMEOUT_MS || '15000');\n\nif (!n8nApiKey) {\n  return {\n    json: {\n      ...item,\n      error: true,\n      error_type: 'config_missing',\n      error_message: 'N8N_API_KEY not configured',\n      action_taken: 'skip_config'\n    }\n  };\n}\n\n// Fonction retry\nasync function fetchWithRetry(url, options, maxRetries) {\n  let lastError;\n  for (let attempt = 0; attempt <= maxRetries; attempt++) {\n    try {\n      const controller = new AbortController();\n      const timeoutId = setTimeout(() => controller.abort(), TIMEOUT_MS);\n      const response = await fetch(url, { ...options, signal: controller.signal });\n      clearTimeout(timeoutId);\n      if (!response.ok) {\n        const errorText = await response.text();\n        throw new Error(`HTTP ${response.status}: ${errorText}`);\n      }\n      return await response.json();\n    } catch (error) {\n      lastError = error;\n      if (attempt < maxRetries) {\n        await new Promise(resolve => setTimeout(resolve, Math.pow(2, attempt) * 1000));\n      }\n    }\n  }\n  throw lastError;\n}\n\ntry {\n  // 1. Chercher le workflow\n  const searchUrl = `${n8nBaseUrl}/api/v1/workflows`;\n  const allWorkflows = await fetchWithRetry(searchUrl, {\n    method: 'GET',\n    headers: {\n      'X-N8N-API-KEY': n8nApiKey,\n      'Content-Type': 'application/json'\n    }\n  }, MAX_RETRIES);\n  \n  const existingWorkflow = allWorkflows.data?.find(w => \n    w.name === workflowNameGuess || \n    w.name.toLowerCase().includes(workflowNameGuess.toLowerCase())\n  );\n  \n  if (!existingWorkflow) {\n    return {\n      json: {\n        ...item,\n        action_taken: 'skip_not_found',\n        workflow_id: null,\n        error: false,\n        message: 'Workflow not found in n8n (already deleted or never existed)'\n      }\n    };\n  }\n  \n  let actionTaken;\n  \n  if (DELETE_ON_REMOVE) {\n    // 2a. Supprimer compl√®tement\n    const deleteUrl = `${n8nBaseUrl}/api/v1/workflows/${existingWorkflow.id}`;\n    await fetchWithRetry(deleteUrl, {\n      method: 'DELETE',\n      headers: {\n        'X-N8N-API-KEY': n8nApiKey\n      }\n    }, MAX_RETRIES);\n    actionTaken = 'deleted';\n    \n  } else {\n    // 2b. D√©sactiver seulement\n    const updateUrl = `${n8nBaseUrl}/api/v1/workflows/${existingWorkflow.id}`;\n    await fetchWithRetry(updateUrl, {\n      method: 'PUT',\n      headers: {\n        'X-N8N-API-KEY': n8nApiKey,\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({\n        ...existingWorkflow,\n        active: false\n      })\n    }, MAX_RETRIES);\n    actionTaken = 'disabled';\n  }\n  \n  return {\n    json: {\n      ...item,\n      action_taken: actionTaken,\n      workflow_id: existingWorkflow.id,\n      workflow_name: existingWorkflow.name,\n      error: false,\n      sync_status: 'success'\n    }\n  };\n  \n} catch (error) {\n  return {\n    json: {\n      ...item,\n      error: true,\n      error_type: 'n8n_disable_failed',\n      error_message: error.message,\n      action_taken: 'failed',\n      sync_status: 'failed'\n    }\n  };\n}"
      },
      "id": "disable-workflow",
      "name": "üóëÔ∏è Disable/Delete Workflow",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1560, 560]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * PREPARE LOG ENTRY\n * - Structure les donn√©es pour audit trail\n * - Ajoute m√©triques de performance\n * - Timestamp pr√©cis\n */\n\nconst item = $input.item.json;\nconst timestamp = new Date().toISOString();\n\nreturn {\n  json: {\n    timestamp_utc: timestamp,\n    event_id: item.event_id || '',\n    execution_id: $execution.id,\n    repo: item.repo || '',\n    branch: item.branch || '',\n    target_env: item.target_env || '',\n    commit_sha: item.commit_sha || '',\n    commit_message: item.commit_message || '',\n    commit_timestamp: item.commit_timestamp || '',\n    actor: item.actor || '',\n    file_path: item.file_path || '',\n    change_type: item.change_type || '',\n    action_taken: item.action_taken || '',\n    workflow_id: item.workflow_id || '',\n    workflow_name: item.workflow_meta?.name || item.workflow_name || '',\n    workflow_url: item.workflow_url || '',\n    node_count: item.workflow_meta?.node_count || 0,\n    has_webhook: item.workflow_meta?.has_webhook || false,\n    has_schedule: item.workflow_meta?.has_schedule || false,\n    sync_status: item.sync_status || 'unknown',\n    error: item.error || false,\n    error_type: item.error_type || '',\n    error_message: item.error_message || '',\n    retry_count: item.retry_count || 0,\n    github_sha: item.github_sha || '',\n    github_size: item.github_size || 0\n  }\n};"
      },
      "id": "prepare-log",
      "name": "üìä Prepare Log Entry",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [2220, 400]
    },
    {
      "parameters": {
        "operation": "append",
        "documentId": {
          "__rl": true,
          "value": "={{ $env.GOOGLE_SHEETS_AUDIT_LOG_ID || '1xEEtkiRFLYvOc0lmK2V6xJyw5jUeye80rqcqjQ2vTpk' }}",
          "mode": "id"
        },
        "sheetName": {
          "__rl": true,
          "value": "github_sync_audit",
          "mode": "name"
        },
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "timestamp_utc": "={{ $json.timestamp_utc }}",
            "event_id": "={{ $json.event_id }}",
            "execution_id": "={{ $json.execution_id }}",
            "repo": "={{ $json.repo }}",
            "branch": "={{ $json.branch }}",
            "target_env": "={{ $json.target_env }}",
            "commit_sha": "={{ $json.commit_sha }}",
            "commit_message": "={{ $json.commit_message }}",
            "actor": "={{ $json.actor }}",
            "file_path": "={{ $json.file_path }}",
            "change_type": "={{ $json.change_type }}",
            "action_taken": "={{ $json.action_taken }}",
            "workflow_id": "={{ $json.workflow_id }}",
            "workflow_name": "={{ $json.workflow_name }}",
            "workflow_url": "={{ $json.workflow_url }}",
            "sync_status": "={{ $json.sync_status }}",
            "error": "={{ $json.error }}",
            "error_type": "={{ $json.error_type }}",
            "error_message": "={{ $json.error_message }}"
          }
        },
        "options": {}
      },
      "id": "log-to-sheets",
      "name": "üìù Log to Sheets",
      "type": "n8n-nodes-base.googleSheets",
      "typeVersion": 4.5,
      "position": [2440, 400],
      "credentials": {
        "googleSheetsOAuth2Api": {
          "id": "google-sheets-creds",
          "name": "Google Sheets OAuth2"
        }
      }
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "leftValue": "={{ $json.error }}",
              "rightValue": "true",
              "operator": {
                "type": "boolean",
                "operation": "true"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "filter-errors",
      "name": "‚ö†Ô∏è Filter Errors Only",
      "type": "n8n-nodes-base.filter",
      "typeVersion": 2,
      "position": [2660, 240]
    },
    {
      "parameters": {
        "authentication": "webhook",
        "select": "channel",
        "channelId": {
          "__rl": true,
          "value": "={{ $env.SLACK_ALERT_CHANNEL_ID || 'C12345' }}",
          "mode": "id"
        },
        "text": "=üö® *GitHub ‚Üí n8n Sync Error*\n\n*Repo:* {{ $json.repo }}\n*Branch:* {{ $json.branch }} ({{ $json.target_env }})\n*File:* `{{ $json.file_path }}`\n*Change:* {{ $json.change_type }}\n*Actor:* {{ $json.actor }}\n\n*Error Type:* `{{ $json.error_type }}`\n*Error Message:*\n```\n{{ $json.error_message }}\n```\n\n*Event ID:* {{ $json.event_id }}\n*Execution:* {{ $json.execution_id }}",
        "otherOptions": {}
      },
      "id": "notify-slack-error",
      "name": "üí¨ Notify Slack (Errors)",
      "type": "n8n-nodes-base.slack",
      "typeVersion": 2.2,
      "position": [2880, 240],
      "credentials": {
        "slackWebhookApi": {
          "id": "slack-webhook",
          "name": "Slack Webhook"
        }
      },
      "notes": "Envoie notification Slack uniquement en cas d'erreur"
    },
    {
      "parameters": {},
      "id": "no-op-skip",
      "name": "‚è≠Ô∏è Skip (No Changes)",
      "type": "n8n-nodes-base.noOp",
      "typeVersion": 1,
      "position": [1560, 680]
    },
    {
      "parameters": {},
      "id": "merge-all-paths",
      "name": "üîó Merge All Paths",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [2220, 400]
    }
  ],
  "connections": {
    "üì• GitHub Webhook": {
      "main": [[{ "node": "üîê Validate & Parse", "type": "main", "index": 0 }]]
    },
    "üîê Validate & Parse": {
      "main": [[{ "node": "‚úÖ Filter Valid Only", "type": "main", "index": 0 }]]
    },
    "‚úÖ Filter Valid Only": {
      "main": [[{ "node": "üìÑ Extract Changed Files", "type": "main", "index": 0 }]]
    },
    "üìÑ Extract Changed Files": {
      "main": [[{ "node": "üîÑ Process One by One", "type": "main", "index": 0 }]]
    },
    "üîÑ Process One by One": {
      "main": [[{ "node": "üîÄ Route by Type", "type": "main", "index": 0 }]]
    },
    "üîÄ Route by Type": {
      "main": [
        [{ "node": "üì• Fetch from GitHub (Retry)", "type": "main", "index": 0 }],
        [{ "node": "üóëÔ∏è Disable/Delete Workflow", "type": "main", "index": 0 }],
        [{ "node": "‚è≠Ô∏è Skip (No Changes)", "type": "main", "index": 0 }]
      ]
    },
    "üì• Fetch from GitHub (Retry)": {
      "main": [[{ "node": "üîç Parse & Validate JSON", "type": "main", "index": 0 }]]
    },
    "üîç Parse & Validate JSON": {
      "main": [[{ "node": "üöÄ Upsert to n8n (Retry)", "type": "main", "index": 0 }]]
    },
    "üöÄ Upsert to n8n (Retry)": {
      "main": [[{ "node": "üîó Merge All Paths", "type": "main", "index": 0 }]]
    },
    "üóëÔ∏è Disable/Delete Workflow": {
      "main": [[{ "node": "üîó Merge All Paths", "type": "main", "index": 0 }]]
    },
    "‚è≠Ô∏è Skip (No Changes)": {
      "main": [[{ "node": "üîó Merge All Paths", "type": "main", "index": 0 }]]
    },
    "üîó Merge All Paths": {
      "main": [[{ "node": "üìä Prepare Log Entry", "type": "main", "index": 0 }]]
    },
    "üìä Prepare Log Entry": {
      "main": [[
        { "node": "üìù Log to Sheets", "type": "main", "index": 0 },
        { "node": "‚ö†Ô∏è Filter Errors Only", "type": "main", "index": 0 }
      ]]
    },
    "üìù Log to Sheets": {
      "main": [[{ "node": "üîÑ Process One by One", "type": "main", "index": 0 }]]
    },
    "‚ö†Ô∏è Filter Errors Only": {
      "main": [[{ "node": "üí¨ Notify Slack (Errors)", "type": "main", "index": 0 }]]
    }
  },
  "settings": {
    "executionOrder": "v1",
    "saveManualExecutions": true,
    "callerPolicy": "workflowsFromSameOwner",
    "errorWorkflow": "{{ $env.ERROR_WORKFLOW_ID || '' }}"
  },
  "staticData": null,
  "tags": ["github", "sync", "automation", "v2026"],
  "triggerCount": 1,
  "updatedAt": "2025-11-22T00:00:00.000Z",
  "versionId": "2.0"
}
